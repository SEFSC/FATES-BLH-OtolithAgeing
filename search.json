[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Menhaden Ageing Model",
    "section": "",
    "text": "This Menhaden Ageing Model provides an innovative method for automatically estimating Menhaden age using scale images. Built upon state-of-the-art deep learning algorithms, it enables rapid generation of fish age predictions by simply pointing to a directory containing magnified images of scale samples."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Menhaden Ageing Model",
    "section": "About",
    "text": "About\nThe internal workflow is as follows:\n\nRaw images are first converted to grayscale such that every image pixel contains a value [0, 255]. These grayscale images are then processed using binary thresholding image processing techniques by which all pixels whose values are above a certain threshold are set to 1 while the rest are set to 0. This allows the scale itself to be distinguished from the image background. The threshold value used for menhaden, based on trail and error, is 100.\nImperfections in the new masked images are cleaned up using morphological opening and closing techniques to remove undesired background noise and capture any missed portions of the scale.\nThe contours of the masked shape are identified in order to extract the object of interest (i.e., the scale).\nThe scale is then cropped out of the original image and padded to make it square.\nThe new square image containing just the scale of interest is passed to a trained custom residual neural network (resnet) deep learning classification model. Model output is saved to a CSV file.\n\nImplementation instructions follow. Be sure to set up and configure a Python environment before the first use."
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Menhaden Ageing Model",
    "section": "Usage",
    "text": "Usage\nRunning the model requires two steps. First, raw images must be pre-processed in order to crop out the scale of interest from the full image, pad the cropped image to ensure the full scale is captured, and (optionally) normalize the cropped image to facilitate ageing. This is all done using the Scale_Raw_Image_Preprocessing.py Python script. To execute, run the following command in a command line terminal:\npython Scale_Raw_Image_Preprocessing.py --config_path &lt;config_dir&gt;\nor\npython Scale_Raw_Image_Preprocessing.py -c &lt;config_dir&gt;\nwhere &lt;config_dir&gt; is the path to the configuration file containing model settings described below. The ageing model itself is wrapped inside a second Python script called Scale_Aging_Inference_Script_Image_Only.py. To execute, run the following command in a command line terminal:\npython Scale_Aging_Inference_Script_Image_Only.py --config_path &lt;config_dir&gt;\nor\npython Scale_Aging_Inference_Script_Image_Only.py -c &lt;config_dir&gt;\n For more information, including an execution example, see Using the Model in the docs."
  },
  {
    "objectID": "index.html#options",
    "href": "index.html#options",
    "title": "Menhaden Ageing Model",
    "section": "Options",
    "text": "Options\nAll user options are contained in a config YAML file (called configurations.yml by default, but can be named anything) to allow easier control and greater reproducibility. Settings are entered as key: value pairs as described below. The first set of parameters control the image processing routine while the second set control the age model itself.\n\nPre-Processing Options\n\nPaths and general options\n\n\n\n\n\n\nKey\nDescription\n\n\n\n\nraw_image_path\nPath to raw images. Best to include the full path in quotations. Example: “G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/images”\n\n\npreprocessed_image_path\nPath to save the processed images. Best to include the full path in quotations and to use a dedicated folder.\n\n\ninput_type\nInput image type\n\n\noutput_type\nOutput image type. Should not need to be changed.\n\n\nsegment\nScale segmentation method: “binary” for binary thresholding and “sam” for Segment Anything Model (SAM). Binary thresholding should work fine if images are high contrast with light scales on a dark background. SAM is more robust to variable image conditions but requires more processing time and a GPU.\n\n\n\n\n\nBinary threshold segmentation parameters\n\n\n\n\n\n\nbinary_threshold\nBinary threshold pixel value for differentiation between foreground (scale) and background. Default: 100\n\n\n\n\n\nSegment Anything Model (SAM) parameters\n\n\n\n\n\n\npoints_per_side\nNumber of points to use for automatic segmentation of scales with SAM. This should be adjusted based on size of object of interest with respect to the entire image. In general, you want number of points to be greater than the ratio of image size/object size for the smallest object of interest. Having too many points though could greatly increase processing time. Default: 16\n\n\nstability_score_thresh\nThreshold for whether to include pixels in object mask. If the mask is too large, increase the score threshold and vice versa if the mask is too small. Default: 0.93\n\n\ndownsample\nDown-sample image size for input to SAM to reduce processing time. Default: 0.5 (i.e. reduce image dimensions to 50% of original size)\n\n\nsam_model_type\nSAM model type. Options are “vit_b”, “vit_l”, and “vit_h” in order of increasing size. Default: “vit_b”\n\n\nsam_weights_path\nPath to SAM model weights. Make sure this matches the model type. Best to use the full path in quotations.\n\n\n\n\n\nCropping and padding parameters\n\n\n\n\n\n\npad\nPadding for top and sides of cropped image. Defined as a fraction of the original cropped image size. Bottom padding is controlled by bottom_pad. Default: 0.2\n\n\nbottom_pad\nPadding for bottom of cropped image. This is defined separately since for scale images, the bottom is usually visually distinct from the body and may be missed in the segmentation. Default: 0.4\n\n\n\n\n\nImage normalization options\n\n\n\n\n\n\nnormalization\nMethod for optionally normalizing the image after cropping and padding. Options: “none”, for no normalization, “he” for histogram equalization, and “clahe” for Contrast Limited Adaptive Histogram Equalization.\n\n\ninvert\nOption to invert the pixel values in gray scale before normalization. This will make dark regions light and light regions dark.\n\n\n\n\n\nAge Model Options\nThe following options control the age inference model. They are entered into the same configuration file for convenience.\n\nConfiguration for age inference model\n\n\n\n\n\n\nKey\nDescription\n\n\n\n\nimage_path\nPath to preprocessed image folder. Best to include the full path in quotations. This should match preprocessed_image_path from the pre-processing step.\n\n\nmodel_path\nPath to model weights (directory and file name of weights file). Best to include the full path in quotations.\n\n\nout_path\nPath to save results (directory and file name for CSV file). Best to include the full path in quotations.\n\n\n\n\n\n\n\n\n\nThe most important settings in this file to pay attention to are:\n\n\n\n\nThe directory containing the scale images to process\nThe directory where you want the model output file to be written\nThe directory containing the trained model weights (i.e., the best_model.pth file) and, if desired, the Segment Anything Model weights. If you simply cloned the repo and have not moved anything around, the trained model weights file will be alongside the model script in the scripts subdirectory in the cloned repository. The SAM weights will be wherever you saved them upon downloading them.\n\n\n\n\n\n\n\n\n\nAbsolute vs. relative file paths\n\n\n\nAbsolute file paths are generally recommended to avoid unintended behavior but will vary from computer to computer."
  },
  {
    "objectID": "index.html#dependencies",
    "href": "index.html#dependencies",
    "title": "Menhaden Ageing Model",
    "section": "Dependencies",
    "text": "Dependencies\n\nAnaconda (for virtual environment implementation)\nGit CLI (for repository cloning)\nPython &gt;= 3.9\ntorch ==1.12.1\ntorchvision == 0.13.1\ntorchaudio == 0.12.1\nopencv-python\npandas\ntqdm\n\n\nRecommendations\n\nmatplotlib (data plotting in Python)\njupyter (viewing Jupyter notebook demonstrations provided in the GitHub repo)\n\nSee Getting Started for installation and setup instructions."
  },
  {
    "objectID": "index.html#release-notes",
    "href": "index.html#release-notes",
    "title": "Menhaden Ageing Model",
    "section": "Release Notes",
    "text": "Release Notes\n\n\n Version History \n\n\n\n2025.0.1 (BETA) (May 2025): Initial version for testing with the following functionality:\n\n\nRun via command line with image, output, and model directories passed as required arguments\n\n\n\n\n\n2025.0.2 (BETA) (July 2025):\n\n\nAdds optional histogram normalization (simple histogram equalization and Contrast Limited Adaptive Histogram Equalization)\n\n\nSeparates image pre-processing and scale ageing into two separate scripts\n\n\nModel settings and hyperparameters, including input and output file paths, are now listed in a configuration YAML file passed as a single command line argument to both Python scripts\n\n\n\n\n\n\n\n\n\n\n\nLicense\n\n\n\nSoftware code created by U.S. Government employees is not subject to copyright in the United States (17 U.S.C. §105). The United States/Department of Commerce reserve all rights to seek and obtain copyright protection in countries other than the United States for Software authored in its entirety by the Department of Commerce. To this end, the Department of Commerce hereby grants to Recipient a royalty-free, nonexclusive license to use, copy, and create derivative works of the Software outside of the United States.\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nThis software is a scientific product and is not official communication of the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All NOAA software and project code is provided on an “as is” basis and the user assumes responsibility for its use. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this software will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government."
  },
  {
    "objectID": "content/docs/mount.html",
    "href": "content/docs/mount.html",
    "title": "Mounting a Drive",
    "section": "",
    "text": "If the data to be processed are stored on a remote drive somewhere, such as in Google Shared Drive, that drive must first be “mounted” in order for it to be accessible via the operating system’s file system. The Google Drive application, which is already installed on the Advanced Technology GPU machine, makes accessing personal Google drives and Google shared drives easy. One must, however, log in to authenticate access.\nSimply double click the Google Drive icon on the desktop and log in as normal. When asked to authenticate with two factor authentication, plug your Yubikey into your local computer and authenticate as usual. The authentication will be passed to the remote machine. Once done, your Google drive will appear in File Explorer as the “G” drive.\nOther drives, such as each lab’s on-prem shared storage drives, are already mapped and are also accessible by their corresponding drive letters.\nThe example in Using the Model illustrates how to access data stored in a Google shared drive.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Mounting a Drive"
    ]
  },
  {
    "objectID": "content/docs/index.html",
    "href": "content/docs/index.html",
    "title": "Menhaden Ageing Model",
    "section": "",
    "text": "This Menhaden Ageing Model provides an innovative method for automatically estimating Menhaden age using scale images. Built upon state-of-the-art deep learning algorithms, it enables rapid generation of fish age predictions by simply pointing to a directory containing magnified images of scale samples.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#about",
    "href": "content/docs/index.html#about",
    "title": "Menhaden Ageing Model",
    "section": "About",
    "text": "About\nThe internal workflow is as follows:\n\nRaw images are first converted to grayscale such that every image pixel contains a value [0, 255]. These grayscale images are then processed using binary thresholding image processing techniques by which all pixels whose values are above a certain threshold are set to 1 while the rest are set to 0. This allows the scale itself to be distinguished from the image background. The threshold value used for menhaden, based on trail and error, is 100.\nImperfections in the new masked images are cleaned up using morphological opening and closing techniques to remove undesired background noise and capture any missed portions of the scale.\nThe contours of the masked shape are identified in order to extract the object of interest (i.e., the scale).\nThe scale is then cropped out of the original image and padded to make it square.\nThe new square image containing just the scale of interest is passed to a trained custom residual neural network (resnet) deep learning classification model. Model output is saved to a CSV file.\n\nImplementation instructions follow. Be sure to set up and configure a Python environment before the first use.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#usage",
    "href": "content/docs/index.html#usage",
    "title": "Menhaden Ageing Model",
    "section": "Usage",
    "text": "Usage\nRunning the model requires two steps. First, raw images must be pre-processed in order to crop out the scale of interest from the full image, pad the cropped image to ensure the full scale is captured, and (optionally) normalize the cropped image to facilitate ageing. This is all done using the Scale_Raw_Image_Preprocessing.py Python script. To execute, run the following command in a command line terminal:\npython Scale_Raw_Image_Preprocessing.py --config_path &lt;config_dir&gt;\nor\npython Scale_Raw_Image_Preprocessing.py -c &lt;config_dir&gt;\nwhere &lt;config_dir&gt; is the path to the configuration file containing model settings described below. The ageing model itself is wrapped inside a second Python script called Scale_Aging_Inference_Script_Image_Only.py. To execute, run the following command in a command line terminal:\npython Scale_Aging_Inference_Script_Image_Only.py --config_path &lt;config_dir&gt;\nor\npython Scale_Aging_Inference_Script_Image_Only.py -c &lt;config_dir&gt;\n For more information, including an execution example, see Using the Model in the docs.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#options",
    "href": "content/docs/index.html#options",
    "title": "Menhaden Ageing Model",
    "section": "Options",
    "text": "Options\nAll user options are contained in a config YAML file (called configurations.yml by default, but can be named anything) to allow easier control and greater reproducibility. Settings are entered as key: value pairs as described below. The first set of parameters control the image processing routine while the second set control the age model itself.\n\nPre-Processing Options\n\nPaths and general options\n\n\n\n\n\n\nKey\nDescription\n\n\n\n\nraw_image_path\nPath to raw images. Best to include the full path in quotations. Example: “G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/images”\n\n\npreprocessed_image_path\nPath to save the processed images. Best to include the full path in quotations and to use a dedicated folder.\n\n\ninput_type\nInput image type\n\n\noutput_type\nOutput image type. Should not need to be changed.\n\n\nsegment\nScale segmentation method: “binary” for binary thresholding and “sam” for Segment Anything Model (SAM). Binary thresholding should work fine if images are high contrast with light scales on a dark background. SAM is more robust to variable image conditions but requires more processing time and a GPU.\n\n\n\n\n\nBinary threshold segmentation parameters\n\n\n\n\n\n\nbinary_threshold\nBinary threshold pixel value for differentiation between foreground (scale) and background. Default: 100\n\n\n\n\n\nSegment Anything Model (SAM) parameters\n\n\n\n\n\n\npoints_per_side\nNumber of points to use for automatic segmentation of scales with SAM. This should be adjusted based on size of object of interest with respect to the entire image. In general, you want number of points to be greater than the ratio of image size/object size for the smallest object of interest. Having too many points though could greatly increase processing time. Default: 16\n\n\nstability_score_thresh\nThreshold for whether to include pixels in object mask. If the mask is too large, increase the score threshold and vice versa if the mask is too small. Default: 0.93\n\n\ndownsample\nDown-sample image size for input to SAM to reduce processing time. Default: 0.5 (i.e. reduce image dimensions to 50% of original size)\n\n\nsam_model_type\nSAM model type. Options are “vit_b”, “vit_l”, and “vit_h” in order of increasing size. Default: “vit_b”\n\n\nsam_weights_path\nPath to SAM model weights. Make sure this matches the model type. Best to use the full path in quotations.\n\n\n\n\n\nCropping and padding parameters\n\n\n\n\n\n\npad\nPadding for top and sides of cropped image. Defined as a fraction of the original cropped image size. Bottom padding is controlled by bottom_pad. Default: 0.2\n\n\nbottom_pad\nPadding for bottom of cropped image. This is defined separately since for scale images, the bottom is usually visually distinct from the body and may be missed in the segmentation. Default: 0.4\n\n\n\n\n\nImage normalization options\n\n\n\n\n\n\nnormalization\nMethod for optionally normalizing the image after cropping and padding. Options: “none”, for no normalization, “he” for histogram equalization, and “clahe” for Contrast Limited Adaptive Histogram Equalization.\n\n\ninvert\nOption to invert the pixel values in gray scale before normalization. This will make dark regions light and light regions dark.\n\n\n\n\n\nAge Model Options\nThe following options control the age inference model. They are entered into the same configuration file for convenience.\n\nConfiguration for age inference model\n\n\n\n\n\n\nKey\nDescription\n\n\n\n\nimage_path\nPath to preprocessed image folder. Best to include the full path in quotations. This should match preprocessed_image_path from the pre-processing step.\n\n\nmodel_path\nPath to model weights (directory and file name of weights file). Best to include the full path in quotations.\n\n\nout_path\nPath to save results (directory and file name for CSV file). Best to include the full path in quotations.\n\n\n\n\n\n\n\n\n\nThe most important settings in this file to pay attention to are:\n\n\n\n\nThe directory containing the scale images to process\nThe directory where you want the model output file to be written\nThe directory containing the trained model weights (i.e., the best_model.pth file) and, if desired, the Segment Anything Model weights. If you simply cloned the repo and have not moved anything around, the trained model weights file will be alongside the model script in the scripts subdirectory in the cloned repository. The SAM weights will be wherever you saved them upon downloading them.\n\n\n\n\n\n\n\n\n\nAbsolute vs. relative file paths\n\n\n\nAbsolute file paths are generally recommended to avoid unintended behavior but will vary from computer to computer.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#dependencies",
    "href": "content/docs/index.html#dependencies",
    "title": "Menhaden Ageing Model",
    "section": "Dependencies",
    "text": "Dependencies\n\nAnaconda (for virtual environment implementation)\nGit CLI (for repository cloning)\nPython &gt;= 3.9\ntorch ==1.12.1\ntorchvision == 0.13.1\ntorchaudio == 0.12.1\nopencv-python\npandas\ntqdm\n\n\nRecommendations\n\nmatplotlib (data plotting in Python)\njupyter (viewing Jupyter notebook demonstrations provided in the GitHub repo)\n\nSee Getting Started for installation and setup instructions.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#release-notes",
    "href": "content/docs/index.html#release-notes",
    "title": "Menhaden Ageing Model",
    "section": "Release Notes",
    "text": "Release Notes\n\n\n Version History \n\n\n\n2025.0.1 (BETA) (May 2025): Initial version for testing with the following functionality:\n\n\nRun via command line with image, output, and model directories passed as required arguments\n\n\n\n\n\n2025.0.2 (BETA) (July 2025):\n\n\nAdds optional histogram normalization (simple histogram equalization and Contrast Limited Adaptive Histogram Equalization)\n\n\nSeparates image pre-processing and scale ageing into two separate scripts\n\n\nModel settings and hyperparameters, including input and output file paths, are now listed in a configuration YAML file passed as a single command line argument to both Python scripts\n\n\n\n\n\n\n\n\n\n\n\nLicense\n\n\n\nSoftware code created by U.S. Government employees is not subject to copyright in the United States (17 U.S.C. §105). The United States/Department of Commerce reserve all rights to seek and obtain copyright protection in countries other than the United States for Software authored in its entirety by the Department of Commerce. To this end, the Department of Commerce hereby grants to Recipient a royalty-free, nonexclusive license to use, copy, and create derivative works of the Software outside of the United States.\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nThis software is a scientific product and is not official communication of the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All NOAA software and project code is provided on an “as is” basis and the user assumes responsibility for its use. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this software will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/acknowledgements.html",
    "href": "content/docs/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This model was created by Aotian Zheng at the University of Washington Information Processing Lab in cooperation with and supported by the NOAA Southeast Fisheries Science Center Fisheries Assessment, Technology, and Engineering Support Division.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Acknowledgments"
    ]
  },
  {
    "objectID": "content/docs/usage.html",
    "href": "content/docs/usage.html",
    "title": "Using the Model",
    "section": "",
    "text": "Running the Menhaden Ageing Model is straightforward and only requires two commands: one to pro-process the images and a second to run the age inference model. You will, however, need to modify a configurations.yml file, as described previously. A sample is provided below, while the contents of this file are explained in more detail in the model options.\n\nActivate the Virtual Environment\nFirst we need to activate the virtual environment we previously configured. In a terminal window, type:\nconda activate scaleageing\n\n\n\n\n\n\nTip\n\n\n\nIf you named your virtual environment something other than “scaleageing”, invoke that name instead.\n\n\n\n\nRunning the model\nNavigate (cd) into the directory containing the Scale_Aging_Inference_Script_Image_Only.py script. If you simply cloned the repo and have not moved anything around, this will in the /Menhaden Scales Aging/Inference Script subdirectory in the cloned repository. For example:\ncd \"c:/Users/user.name/Documents/ageing/FATES-BLH-OtolithAgeing/Menhaden Scales Aging/Inference Script\"\nRun the model as described in the usage instructions:\npython Scale_Raw_Image_Preprocessing.py --config_path &lt;config_dir&gt;\npython Scale_Aging_Inference_Script_Image_Only.py --config_path &lt;config_dir&gt;\nwhere &lt;config_dir&gt; is the path to the configuration file containing model settings presented above.\n\nExamples\nIf your configuration.yml file is stored alongside your model script, your execution commands will simply be:\npython Scale_Raw_Image_Preprocessing.py --config_path configuration.yml\npython Scale_Aging_Inference_Script_Image_Only.py --config_path configuration.yml\nOr, if you named your configuration file something unique such as configuration-2024-atl.yml, it might look like this:\npython Scale_Raw_Image_Preprocessing.py --config_path configuration-2024-atl.yml\npython Scale_Aging_Inference_Script_Image_Only.py --config_path configuration-2024-atl.yml\nAlternatively, your configuration.yml file is stored alongside your data, you will need to tell the script exactly where that file is located using its full directory path. Be sure to also include the correct file name, if applicable. For example:\npython Scale_Raw_Image_Preprocessing.py --config_path \"c:/Users/user.name/Documents/data/2024/configuration.yml\"\npython Scale_Aging_Inference_Script_Image_Only.py --config_path \"c:/Users/user.name/Documents/data/2024/configuration.yml\"\n\n\n\n\n\n\nTip\n\n\n\nIf you get a “File not found” error when running the script, carefully check your directories and file names, including those contained in the configuration file.\n\n\n\n\n\nWhen finished\nWhen you are finished running the model, deactivate the virtual environment to free up resources on your computer:\nconda deactivate\nNote that there is no need to specify the environment when deactivating, as conda automatically deactivates the running environment.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Using the Model"
    ]
  },
  {
    "objectID": "content/docs/setup.html",
    "href": "content/docs/setup.html",
    "title": "Getting Started",
    "section": "",
    "text": "Note\n\n\n\nThe instructions on this page only need to be carried out once. If you have already installed the required dependencies and created a virtual environment, skip ahead to Using the Model.\n\n\nMuch of the following is carried out using command line. In the instructions below, “command line terminal” or “terminal” refer to any command line application for the given operating system. For Windows, this is commonly Command Prompt or Windows Powershell. These use DOS commands. For UNIX users, Git Bash is a good alternative; it is included in Git for Windows and allows UNIX commands to be used instead of DOS.\n\nSoftware Installation\n\ngit\nIf the Git CLI is already installed, there should be a Git Bash program in your Applications. If so, open this program and verify everything is working by typing\ngit --version\nThis should display the version of git installed on the system.\n\n\n\n\n\n\nNote\n\n\n\nThis should already be installed for all users on the Advanced Technology GPU machine.\n\n\nIf needed, download and install the Git CLI before continuing. This does not need administrative privileges to install at the user level. Confirm that it has been installed by opening a command line terminal and entering git --version.\n\n\nconda\nDownload and install Anaconda.\n\n\n\n\n\n\nNote\n\n\n\nThis is already installed on the Advanced Technology GPU machine, but it may need to be set up for each user. Keep reading to learn more.\n\n\nVerify installation and configuration of Anaconda using a command line terminal:\nconda --version\nIf Anaconda is configured properly, this should print the version of the installed software. If, instead, it returns a “command not found” error, and you know Anaconda has been installed, you most likely need to add Conda to your path variables so that your operating system knows where to find it. First, find where it was installed, and then, in Windows:\n\nIn the Windows taskbar, search for “environment” and select “Edit environmental variables for your account”\nIn the top panel of the window that opens, click the “Path” user variable, then select “Edit…”.\nYou need to add two new paths, one at a time. Click “New” and enter the full path (starting with “C:\") of the directory containing Anaconda. This may, for example, be C:\\Users\\user.name\\AppData\\Local\\anaconda3, if installed at the user level (where user.name is your user name), or C:\\ProgramData\\anaconda3 if installed for all users.\nRepeat step 3 with the same path but with \\Scripts\\ appended to the end. For example, C:\\ProgramData\\anaconda3\\Scripts\\.\nClick Ok (twice) to save and exit Settings.\n\nClose and reopen your command line terminal. Then type conda init to initialize Anaconda. You may need to close and reopen your command line terminal again (it will prompt you if needed.) Finally, try the conda --version command again. It should work now.\n\n\n\nClone the Repository\nThe model code and most of its dependencies are available on GitHub. To retrieve it, clone the scales branch of this repo, which is currently functioning as the production version of the model:\n\nOpen a terminal window and change directories (cd) to wherever you want the code to reside. For example,\ncd Documents/ageing\n\n\n\n\n\n\nImportant\n\n\n\nMake sure the directory exists before trying to navigate into it. You cannot move into a non-existent folder.\n\n\nClone the repository into the desired directory\ngit clone -b scales https://github.com/SEFSC/FATES-BLH-OtolithAgeing .\nif you want all of the repo contents in the current directory, or\ngit clone -b scales https://github.com/SEFSC/FATES-BLH-OtolithAgeing\nif you want the repo contents to be downloaded into a new subdirectory called FATES-BLH-OtolithAgeing. This is just personal preference. (Note the difference between the two commands is the dot . at the end.)\n\n\n\nDownload Segmentation Model (recommended)\nThis model has the option of using a Segment Anything Model (SAM) to find the scale within each image. To use this method, one must download a model checkpoint. The ViT-B model is recommended. For convenience, consider saving this file in the same directory as model script, for example ~/FATES-BLH-OtolithAgeing/Menhaden Scales Aging/Inference Script.\n\n\n\n\n\n\nImportant\n\n\n\nYou only need to download this if you intend to use Segment Anything Model method to image preprocessing. See Using the Model for more information.\n\n\n\n\nCreate a Virtual Environment\nVirtual environments are used to manage Python packages. Create a virtual environment specifically for this model. In a terminal window, navigate (cd) into the directory containing the environment.yml file. This should be the parent directory of the cloned repository. For example,\ncd \"c:/Users/user.name/Documents/ageing/FATES-BLH-OtolithAgeing\"\nwhere user.name is your computer user name.\nThen:\nconda env create --name scaleageing --file=\"environment.yml\"\nThis command creates a virtual environment called scaleageing and downloads and installs all of the package dependencies listed in the environment.yml file contained in the repo. The preceding ./ means that file is located “here” in the present working directory to which you navigated in the previous step.\nNote that this virtual environment can be named anything at all, but remember what you call it – and keep it simple – because it will be invoked by name each time you use the model.\n\n\n\n\n\n\nDeleting a virtual environment\n\n\n\nIn the unlikely event you need to delete this virtual environment, such as to troubleshoot or just need to restart from a clean slate, it can be deleted using:\nconda remove --name ENV_NAME --all\nwhere ENV_NAME is the name of the virtual environment to be deleted (e.g., scaleageing.)\n\n\nCongratulations! Now that your environment is set up – and this only needs to be done once, the first time – we are ready to start using the model.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/docs/configuration.html",
    "href": "content/docs/configuration.html",
    "title": "Configuring the Model",
    "section": "",
    "text": "A configurations.yml file is an easy way to control model performance. The most important settings in this file to pay attention to are:\n\nThe directory containing the scale images to process\nThe directory where you want the model output file to be written\nThe directory containing the trained model weights (i.e., the best_model.pth file) and, if desired, the Segment Anything Model weights. If you simply cloned the repo and have not moved anything around, the trained model weights file will be alongside the model script in the scripts subdirectory in the cloned repository. The SAM weights will be wherever you saved them upon downloading them.\n\nAbsolute file paths are generally recommended to avoid unintended behavior but will vary from computer to computer.\nThis YAML file is structured as key: value pairs. The order in which these entries are presented in the file does not matter, but all keys need to be included and match exactly as expected. For example:\n\n\nconfiguration.yml\n\n# --------------------------------------------------------------------------------------------\n# Configuration for pre-processsing scale images (crop, pad, and normalization)\n# --------------------------------------------------------------------------------------------\n\n# -----Paths and general options-----\nraw_image_path: \"G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/images\"\npreprocessed_image_path: \"G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/cropped\"\ninput_type: \".tif\"\noutput_type: \".jpg\"\nsegment: \"sam\"\n\n# -----Binary Threshold segmentation parameters-----\nbinary_threshold: 100\n\n# -----Segment Anything Model (SAM) parameters-----\npoints_per_side: 16\nstability_score_thresh: 0.93\ndownsample: 0.5\nsam_model_type: \"vit_b\"\nsam_weights_path: \"C:/Users/matt.grossi/Documents/GitHubRepos/FATES-BLH-OtolithAgeing/Menhaden Scales Aging/Inference Script/sam_vit_b_01ec64.pth\"\n\n# -----Cropping and padding parameters-----\npad: 0.2\nbottom_pad: 0.4\n\n# -----Normalization options-----\nnormalization: \"clahe\"\ninvert: True\n\n# --------------------------------------------------------------------------------------------\n# Configuration for age inference \n# --------------------------------------------------------------------------------------------\n\n# -----Model paths-----\nimage_path: \"G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/cropped\"\nmodel_path: \"C:/Users/matt.grossi/Documents/GitHubRepos/FATES-BLH-OtolithAgeing/Menhaden Scales Aging/Chapter 2 - Inference on Images/best_model.pth\"\nout_path: \"G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/model_predictions/inference_results.csv\"\n\n\n\n\n\n\n\nA note on reproducibility\n\n\n\nFor the sake of documenting workflows and facilitating future reproducibility, consider creating new configuration files for each model run (for example, one for each data set to be processed). There is no restriction on what this file can be called; you will tell the model which file to use when you execute the script. Thus, some convention like configuration-2024-atl.yml might be sensible.\n\n\n\nWhere to save your configuration file\nThere are two schools of thought when it comes to organizing the model files. It is ultimately up to the user to choose whichever convention is best for them.\n\nOption 1: Alongside the model\nStoring the configuration.yml file in the same directory as the model scripts is advantageous when running the model because you will not need to include the full directory path when you specify which configuration file to use. Since you will be executing the model script from the directory that script resides, the system will automatically find the configuration.yml file in that same directory.\nThe disadvantage to this option is that your directory may quickly become cluttered with different configuration files for different model runs.\n\n\nOption 2: Alongside the data\nOne might opt instead to store the configuration.yml file in the same directory as the data to be processed or the directory where the model output will be written out. This is helpful for documenting workflows and ensuring reproducibility since it will be easy to see how a given data set was processed.\nThe disadvantage to this option is that it will require the full directory path to be included when running the model and specifying the configuration file to use.\n\n\n\n\n\n\nNote\n\n\n\nThese are by no means the only options. Whatever convention is adopted, consistency is key. You future self will thank you some day.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Configuring the Model"
    ]
  }
]
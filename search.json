[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Menhaden Ageing Model",
    "section": "",
    "text": "This Menhaden Ageing Model provides an innovative method for automatically estimating Menhaden age using scale images. Built upon state-of-the-art deep learning algorithms, it enables rapid generation of fish age predictions by simply pointing to a directory containing magnified images of scale samples."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Menhaden Ageing Model",
    "section": "About",
    "text": "About\nThe internal workflow is as follows:\n\nRaw images are first converted to grayscale such that every image pixel contains a value [0, 255]. These grayscale images are then processed using binary thresholding image processing techniques by which all pixels whose values are above a certain threshold are set to 1 while the rest are set to 0. This allows the scale itself to be distinguished from the image background. The threshold value used for menhaden, based on trail and error, is 100.\nImperfections in the new masked images are cleaned up using morphological opening and closing techniques to remove undesired background noise and capture any missed portions of the scale.\nThe contours of the masked shape are identified in order to extract the object of interest (i.e., the scale).\nThe scale is then cropped out of the original image and padded to make it square.\nThe new square image containing just the scale of interest is passed to a trained custom residual neural network (resnet) deep learning classification model. Model output is saved to a CSV file.\n\nImplementation instructions follow. Be sure to set up and configure a Python environment before the first use."
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Menhaden Ageing Model",
    "section": "Usage",
    "text": "Usage\nRunning the model requires two steps. First, raw images must be pre-processed in order to crop out the scale of interest from the full image, pad the cropped image to ensure the full scale is captured, and (optionally) normalize the cropped image to facilitate ageing. This is all done using the Scale_Raw_Image_Preprocessing.py Python script. To execute, run the following command in a command line terminal:\npython Scale_Raw_Image_Preprocessing.py --config-file &lt;config_dir&gt;\nor\npython Scale_Raw_Image_Preprocessing.py -c &lt;config_dir&gt;\nwhere &lt;config_dir&gt; is the path to the configuration file containing model settings described below. The ageing model itself is wrapped inside a second Python script called Scale_Aging_Inference_Script_Image_Only.py. To execute, run the following command in a command line terminal:\npython Scale_Aging_Inference_Script_Image_Only.py --config-file &lt;config_dir&gt;\nor\npython Scale_Aging_Inference_Script_Image_Only.py -c &lt;config_dir&gt;\n For more information, including an execution example, see Using the Model in the docs."
  },
  {
    "objectID": "index.html#options",
    "href": "index.html#options",
    "title": "Menhaden Ageing Model",
    "section": "Options",
    "text": "Options\nAll user options are contained in a config YAML file (called configurations.yml by default, but can be named anything) to allow easier control and greater reproducibility. Settings are entered as key: value pairs as described below. The first set of parameters control the image processing routine while the second set control the age model itself.\n\nPre-Processing Options\n\nPaths and general options\n\n\n\n\n\n\n\nKey\nDescription\nNotes\n\n\n\n\nraw_image_path\nPath to raw images\nBest to include the full path in quotations. Example: “G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/images”\n\n\npreprocessed_image_path\nPath to save the processed images\nBest to include the full path in quotations and to use a dedicated folder. Example: “G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/cropped”\n\n\ninput_type\nInput image type\n\n\n\noutput_type\nOutput image type\nShould not need to be changed\n\n\nsegment\nScale segmentation method: “binary” for binary thresholding and “sam” for Segment Anything Model (SAM)\nBinary thresholding should work fine if images are high contrast with light scales on a dark background. SAM is more robust to variable image conditions but requires more processing time and a GPU.\n\n\n\n\nBinary threshold segmentation parameters\n\n\n\n\n\n\n\nbinary_threshold\nBinary threshold pixel value for differentiation between foreground (scale) and background\nDefault: 100\n\n\n\n\nSegment Anything Model (SAM) parameters\n\n\n\n\n\n\n\npoints_per_side\nNumber of points to use for automatic segmentation of scales with SAM\nThis should be adjusted based on size of object of interest with respect to the entire image. In general, you want number of points to be greater than the ratio of image size/object size for the smallest object of interest. Having too many points though could greatly increase processing time. Default: 16\n\n\nstability_score_thresh\nThreshold for whether to include pixels in object mask\nIf the mask is too large, increase the score threshold and vice versa if the mask is too small. Default: 0.93\n\n\ndownsample\nDown-sample image size for input to SAM to reduce processing time\nDefault: 0.5 (i.e. reduce image dimensions to 50% of original size)\n\n\nsam_model_type\nSAM model type. Options are “vit_b”, “vit_l”, and “vit_h” in order of increasing size\nDefault: “vit_b”\n\n\nsam_weights_path\nPath to SAM model weights. Make sure this matches the model type\nBest to use the full path in quotations.\n\n\n\n\nCropping and padding parameters\n\n\n\n\n\n\n\npad\nPadding for top and sides of cropped image. Defined as a fraction of the original cropped image size\nBottom padding is controlled by bottom_pad. Default: 0.2\n\n\nbottom_pad\nPadding for bottom of cropped image\nThis is defined separately since for scale images, the bottom is usually visually distinct from the body and may be missed in the segmentation. Default: 0.4\n\n\n\n\nImage normalization options\n\n\n\n\n\n\n\nnormalization\nMethod for optionally normalizing the image after cropping and padding\nOptions: “none”, for no normalization, “he” for histogram equalization, and “clahe” for Contrast Limited Adaptive Histogram Equalization.\n\n\ninvert\nOption to invert the pixel values in gray scale before normalization\nThis will make dark regions light and light regions dark.\n\n\n\n\n\nAge Model Options\nThe following options control the age inference model. They are entered into the same configuration file for convenience.\n\nConfiguration for age inference model\n\n\n\n\n\n\n\nKey\nDescription\nNotes\n\n\n\n\nimage_path\nPath to preprocessed image folder\nBest to include the full path in quotations. This should match preprocessed_image_path from the pre-processing step.\n\n\nmodel_path\nPath to model weights (directory and file name of weights file)\nBest to include the full path in quotations.\n\n\nout_path\nPath to save results (directory and file name for CSV file)\nBest to include the full path in quotations.\n\n\n\n\n\n\n\n\n\nThe most important settings in this file to pay attention to are:\n\n\n\n\nThe directory containing the scale images to process\nThe directory where you want the model output file to be written\nThe directory containing the trained model weights (i.e., the best_model.pth file) and, if desired, the Segment Anything Model weights. If you simply cloned the repo and have not moved anything around, the trained model weights file will be alongside the model script in the scripts subdirectory in the cloned repository. The SAM weights will be wherever you saved them upon downloading them.\n\n\n\n\n\n\n\n\n\nAbsolute vs. relative file paths\n\n\n\nAbsolute file paths are generally recommended to avoid unintended behavior but will vary from computer to computer."
  },
  {
    "objectID": "index.html#dependencies",
    "href": "index.html#dependencies",
    "title": "Menhaden Ageing Model",
    "section": "Dependencies",
    "text": "Dependencies\n\nAnaconda (for virtual environment implementation)\nGit CLI (for repository cloning)\nPython &gt;= 3.9\ntorch ==1.12.1\ntorchvision == 0.13.1\ntorchaudio == 0.12.1\nopencv-python\npandas\ntqdm\n\n\nRecommendations\n\nmatplotlib (data plotting in Python)\njupyter (viewing Jupyter notebook demonstrations provided in the GitHub repo)\n\nSee Getting Started for installation and setup instructions."
  },
  {
    "objectID": "index.html#release-notes",
    "href": "index.html#release-notes",
    "title": "Menhaden Ageing Model",
    "section": "Release Notes",
    "text": "Release Notes\n\n\n Version History \n\n\n\n2025.0.1 (BETA) (May 2025): Initial version for testing with the following functionality:\n\n\nRun via command line with image, output, and model directories passed as required arguments\n\n\n&lt;/li&gt;\n&lt;li&gt; &lt;b&gt;2025.0.2 (BETA)&lt;/b&gt; (July 2025):\n  &lt;ul&gt;\n    &lt;li&gt; Adds optional histogram normalization (simple histogram equalization and Contrast Limited Adaptive Histogram Equalization) &lt;/li&gt;\n    &lt;li&gt; Separates image pre-processing and scale ageing into two separate scripts &lt;/li&gt;\n    &lt;li&gt; Model settings and hyperparameters, including input and output file paths, are now listed in a configuration YAML file passed as a single command line argument to both Python scripts &lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/li&gt;\n\n\n\n\n\n\n\n\n\nLicense\n\n\n\nSoftware code created by U.S. Government employees is not subject to copyright in the United States (17 U.S.C. §105). The United States/Department of Commerce reserve all rights to seek and obtain copyright protection in countries other than the United States for Software authored in its entirety by the Department of Commerce. To this end, the Department of Commerce hereby grants to Recipient a royalty-free, nonexclusive license to use, copy, and create derivative works of the Software outside of the United States.\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nThis software is a scientific product and is not official communication of the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All NOAA software and project code is provided on an “as is” basis and the user assumes responsibility for its use. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this software will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government."
  },
  {
    "objectID": "content/docs/mount.html",
    "href": "content/docs/mount.html",
    "title": "Mounting a Drive",
    "section": "",
    "text": "If the data to be processed are stored on a remote drive somewhere, such as in Google Shared Drive, that drive must first be “mounted” in order for it to be accessible via the operating system’s file system. The Google Drive application, which is already installed on the Advanced Technology GPU machine, makes accessing personal Google drives and Google shared drives easy. One must, however, log in to authenticate access.\nSimply double click the Google Drive icon on the desktop and log in as normal. When asked to authenticate with two factor authentication, plug your Yubikey into your local computer and authenticate as usual. The authentication will be passed to the remote machine. Once done, your Google drive will appear in File Explorer as the “G” drive.\nOther drives, such as each lab’s on-prem shared storage drives, are already mapped and are also accessible by their corresponding drive letters.\nThe example in Using the Model illustrates how to access data stored in a Google shared drive.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Mounting a Drive"
    ]
  },
  {
    "objectID": "content/docs/setup.html",
    "href": "content/docs/setup.html",
    "title": "Getting Started",
    "section": "",
    "text": "Note\n\n\n\nThe instructions on this page only need to be carried out once. If you have already installed the required dependencies and created a virtual environment, skip ahead to Using the Model.\n\n\nMuch of the following is carried out using command line. In the instructions below, “command line terminal” or “terminal” refer to any command line application for the given operating system. For Windows, this is commonly Command Prompt or Windows Powershell. These use DOS commands. For UNIX users, Git Bash is a good alternative; it is included in Git for Windows and allows UNIX commands to be used instead of DOS.\n\nSoftware Installation\n\ngit\nIf the Git CLI is already installed, there should be a Git Bash program in your Applications. If so, open this program and verify everything is working by typing\ngit --version\nThis should display the version of git installed on the system.\n\n\n\n\n\n\nNote\n\n\n\nThis should already be installed for all users on the Advanced Technology GPU machine.\n\n\nIf needed, download and install the Git CLI before continuing. This does not need administrative privileges to install at the user level. Confirm that it has been installed by opening a command line terminal and entering git --version.\n\n\nconda\nDownload and install Anaconda.\n\n\n\n\n\n\nNote\n\n\n\nThis is already installed on the Advanced Technology GPU machine, but it may need to be set up for each user. Keep reading to learn more.\n\n\nVerify installation and configuration of Anaconda using a command line terminal:\nconda --version\nIf Anaconda is configured properly, this should print the version of the installed software. If, instead, it returns a “command not found” error, and you know Anaconda has been installed, you most likely need to add Conda to your path variables so that your operating system knows where to find it. First, find where it was installed, and then, in Windows:\n\nIn the Windows taskbar, search for “environment” and select “Edit environmental variables for your account”\nIn the top panel of the window that opens, click the “Path” user variable, then select “Edit…”.\nYou need to add two new paths, one at a time. Click “New” and enter the full path (starting with “C:\") of the directory containing Anaconda. This may, for example, be C:\\Users\\user.name\\AppData\\Local\\anaconda3, if installed at the user level (where user.name is your user name), or C:\\ProgramData\\anaconda3 if installed for all users.\nRepeat step 3 with the same path but with \\Scripts\\ appended to the end. For example, C:\\ProgramData\\anaconda3\\Scripts\\.\nClick Ok (twice) to save and exit Settings.\n\nClose and reopen your command line terminal. Then type conda init to initialize Anaconda. Finally, try the conda --version command again. It should work now.\n\n\n\nClone the Repository\nMost of the model code is available on GitHub. To retrieve it:\n\nOpen a terminal window and change directories (cd) to wherever you want the code to reside. For example,\ncd Documents/ageing\n\n\n\n\n\n\nImportant\n\n\n\nMake sure the directory exists before trying to navigate into it. You cannot move into a non-existent folder.\n\n\nClone the repository into the desired directory\ngit clone https://github.com/SEFSC/FATES-BLH-OtolithAgeing .\nif you want all of the repo contents in the current directory, or\ngit clone https://github.com/SEFSC/FATES-BLH-OtolithAgeing\nif you want the repo contents to be downloaded into a new subdirectory called FATES-BLH-OtolithAgeing. This is just personal preference. (Note the difference between the two commands is the dot . at the end.)\n\nThis model has the option of using a Segment Anything Model (SAM) to find the scale within each image. To use this method, one must download a model checkpoint. The ViT-B model is recommended. For convenience, consider saving this file in the same directory as model script, for example ~/FATES-BLH-OtolithAgeing/Menhaden Scales Aging/Inference Script.\n\n\nCreate a Virtual Environment\nVirtual environments are used to manage Python packages. Create a virtual environment specifically for this model. In a terminal window, navigate into the directory containing the model code (from the previous step). Then:\nconda env create --name scaleageing --file=environment.yml\nThis command creates a virtual environment called scaleageing and downloads and installs all of the package dependencies listed in the environment.yml file contained in the repo.\nNote that this virtual environment can be named anything at all, but remember what you call it – and keep it simple – because it will be invoked by name each time you use the model.\nCongratulations! Now that your environment is set up – and this only needs to be done once, the first time – we are ready to start using the model.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/docs/acknowledgements.html",
    "href": "content/docs/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This model was created by Aotian Zheng at the University of Washington Information Processing Lab in cooperation with and supported by the NOAA Southeast Fisheries Science Center Fisheries Assessment, Technology, and Engineering Support Division.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Acknowledgments"
    ]
  },
  {
    "objectID": "content/docs/usage.html",
    "href": "content/docs/usage.html",
    "title": "Using the Model",
    "section": "",
    "text": "Running the Menhaden Ageing Model is straightforward and only requires one command. You will, however, need to modify a configurations.yml file. This is explained in more detail below.\n\nConfiguration File\nA configurations.yml file is an easy way to control model performance. The most important settings in this file to pay attention to are:\n\nThe directory containing the scale images to process\nThe directory where you want the model output file to be written\nThe directory containing the trained model weights (i.e., the best_model.pth file) and, if desired, the Segment Anything Model weights. If you simply cloned the repo and have not moved anything around, the trained model weights file will be alongside the model script in the scripts subdirectory in the cloned repository. The SAM weights will be wherever you saved them upon downloading them.\n\nAbsolute file paths are recommended…\nTo be continued.\n\n\nRunning the model\nFirst we need to activate the virtual environment we previously configured. In a terminal window, type:\nconda activate scaleageing\n\n\n\n\n\n\nTip\n\n\n\nIf you named your virtual environment something other than “scaleageing”, invoke that name instead.\n\n\nNavigate into the directory containing the Scale_Aging_Inference_Script_Image_Only.py script. If you simply cloned the repo and have not moved anything around, this will in the scripts subdirectory in the cloned repository.\nRun the model as described in the usage instructions:\npython Scale_Aging_Inference_Script_Image_Only.py &lt;img_dir&gt; &lt;csv_dir&gt; &lt;model_dir&gt;\nwhere &lt;img_dir&gt; is the full path to the directory containing the scale images to process (#1 above), &lt;csv_dir&gt; is the full path to desired directory to which the CSV output will be written (#2 above), and &lt;model_dir&gt; is the full path to the directory containing the best_model.pth model weights (#3 above).\n\n\nAn Example\nDirectories are handled slightly differently in DOS (Windows Command Prompt, Powershell) than UNIX (Git Bash). For example, running on the Advanced Technology GPU machine might to process 2024 menhaden data might look like this:\n\nGit Bash\npython Scale_Aging_Inference_Script_Image_Only.py \"G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/Atlantic menhaden to be tested w active learning/subset\" \"G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/Atlantic menhaden to be tested w active learning/model_age_predictions\" \"C:/Users/user.name/Documents/GitHubRepos/FATES-BLH-ScaleAgeing/scripts\"\n\n\nCommand Prompt\npython Scale_Aging_Inference_Script_Image_Only.py “G:\\Shared drives\\NMFS SEFSC FATES Advanced Technology\\BIOLOGY_LIFE_HISTORY_DATA\\Atlantic menhaden to be tested w active learning\\subset” “C:\\Shared drives\\NMFS SEFSC FATES Advanced Technology\\BIOLOGY_LIFE_HISTORY_DATA\\Atlantic menhaden to be tested w active learning\\model_age_predictions” “C:\\Users\\user.name\\Documents\\GitHubRepos\\FATES-BLH-ScaleAgeing\\scripts”",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Using the Model"
    ]
  },
  {
    "objectID": "content/docs/index.html",
    "href": "content/docs/index.html",
    "title": "Menhaden Ageing Model",
    "section": "",
    "text": "This Menhaden Ageing Model provides an innovative method for automatically estimating Menhaden age using scale images. Built upon state-of-the-art deep learning algorithms, it enables rapid generation of fish age predictions by simply pointing to a directory containing magnified images of scale samples.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#about",
    "href": "content/docs/index.html#about",
    "title": "Menhaden Ageing Model",
    "section": "About",
    "text": "About\nThe internal workflow is as follows:\n\nRaw images are first converted to grayscale such that every image pixel contains a value [0, 255]. These grayscale images are then processed using binary thresholding image processing techniques by which all pixels whose values are above a certain threshold are set to 1 while the rest are set to 0. This allows the scale itself to be distinguished from the image background. The threshold value used for menhaden, based on trail and error, is 100.\nImperfections in the new masked images are cleaned up using morphological opening and closing techniques to remove undesired background noise and capture any missed portions of the scale.\nThe contours of the masked shape are identified in order to extract the object of interest (i.e., the scale).\nThe scale is then cropped out of the original image and padded to make it square.\nThe new square image containing just the scale of interest is passed to a trained custom residual neural network (resnet) deep learning classification model. Model output is saved to a CSV file.\n\nImplementation instructions follow. Be sure to set up and configure a Python environment before the first use.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#usage",
    "href": "content/docs/index.html#usage",
    "title": "Menhaden Ageing Model",
    "section": "Usage",
    "text": "Usage\nRunning the model requires two steps. First, raw images must be pre-processed in order to crop out the scale of interest from the full image, pad the cropped image to ensure the full scale is captured, and (optionally) normalize the cropped image to facilitate ageing. This is all done using the Scale_Raw_Image_Preprocessing.py Python script. To execute, run the following command in a command line terminal:\npython Scale_Raw_Image_Preprocessing.py --config-file &lt;config_dir&gt;\nor\npython Scale_Raw_Image_Preprocessing.py -c &lt;config_dir&gt;\nwhere &lt;config_dir&gt; is the path to the configuration file containing model settings described below. The ageing model itself is wrapped inside a second Python script called Scale_Aging_Inference_Script_Image_Only.py. To execute, run the following command in a command line terminal:\npython Scale_Aging_Inference_Script_Image_Only.py --config-file &lt;config_dir&gt;\nor\npython Scale_Aging_Inference_Script_Image_Only.py -c &lt;config_dir&gt;\n For more information, including an execution example, see Using the Model in the docs.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#options",
    "href": "content/docs/index.html#options",
    "title": "Menhaden Ageing Model",
    "section": "Options",
    "text": "Options\nAll user options are contained in a config YAML file (called configurations.yml by default, but can be named anything) to allow easier control and greater reproducibility. Settings are entered as key: value pairs as described below. The first set of parameters control the image processing routine while the second set control the age model itself.\n\nPre-Processing Options\n\nPaths and general options\n\n\n\n\n\n\n\nKey\nDescription\nNotes\n\n\n\n\nraw_image_path\nPath to raw images\nBest to include the full path in quotations. Example: “G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/images”\n\n\npreprocessed_image_path\nPath to save the processed images\nBest to include the full path in quotations and to use a dedicated folder. Example: “G:/Shared drives/NMFS SEFSC FATES Advanced Technology/BIOLOGY_LIFE_HISTORY_DATA/age_testing/cropped”\n\n\ninput_type\nInput image type\n\n\n\noutput_type\nOutput image type\nShould not need to be changed\n\n\nsegment\nScale segmentation method: “binary” for binary thresholding and “sam” for Segment Anything Model (SAM)\nBinary thresholding should work fine if images are high contrast with light scales on a dark background. SAM is more robust to variable image conditions but requires more processing time and a GPU.\n\n\n\n\nBinary threshold segmentation parameters\n\n\n\n\n\n\n\nbinary_threshold\nBinary threshold pixel value for differentiation between foreground (scale) and background\nDefault: 100\n\n\n\n\nSegment Anything Model (SAM) parameters\n\n\n\n\n\n\n\npoints_per_side\nNumber of points to use for automatic segmentation of scales with SAM\nThis should be adjusted based on size of object of interest with respect to the entire image. In general, you want number of points to be greater than the ratio of image size/object size for the smallest object of interest. Having too many points though could greatly increase processing time. Default: 16\n\n\nstability_score_thresh\nThreshold for whether to include pixels in object mask\nIf the mask is too large, increase the score threshold and vice versa if the mask is too small. Default: 0.93\n\n\ndownsample\nDown-sample image size for input to SAM to reduce processing time\nDefault: 0.5 (i.e. reduce image dimensions to 50% of original size)\n\n\nsam_model_type\nSAM model type. Options are “vit_b”, “vit_l”, and “vit_h” in order of increasing size\nDefault: “vit_b”\n\n\nsam_weights_path\nPath to SAM model weights. Make sure this matches the model type\nBest to use the full path in quotations.\n\n\n\n\nCropping and padding parameters\n\n\n\n\n\n\n\npad\nPadding for top and sides of cropped image. Defined as a fraction of the original cropped image size\nBottom padding is controlled by bottom_pad. Default: 0.2\n\n\nbottom_pad\nPadding for bottom of cropped image\nThis is defined separately since for scale images, the bottom is usually visually distinct from the body and may be missed in the segmentation. Default: 0.4\n\n\n\n\nImage normalization options\n\n\n\n\n\n\n\nnormalization\nMethod for optionally normalizing the image after cropping and padding\nOptions: “none”, for no normalization, “he” for histogram equalization, and “clahe” for Contrast Limited Adaptive Histogram Equalization.\n\n\ninvert\nOption to invert the pixel values in gray scale before normalization\nThis will make dark regions light and light regions dark.\n\n\n\n\n\nAge Model Options\nThe following options control the age inference model. They are entered into the same configuration file for convenience.\n\nConfiguration for age inference model\n\n\n\n\n\n\n\nKey\nDescription\nNotes\n\n\n\n\nimage_path\nPath to preprocessed image folder\nBest to include the full path in quotations. This should match preprocessed_image_path from the pre-processing step.\n\n\nmodel_path\nPath to model weights (directory and file name of weights file)\nBest to include the full path in quotations.\n\n\nout_path\nPath to save results (directory and file name for CSV file)\nBest to include the full path in quotations.\n\n\n\n\n\n\n\n\n\nThe most important settings in this file to pay attention to are:\n\n\n\n\nThe directory containing the scale images to process\nThe directory where you want the model output file to be written\nThe directory containing the trained model weights (i.e., the best_model.pth file) and, if desired, the Segment Anything Model weights. If you simply cloned the repo and have not moved anything around, the trained model weights file will be alongside the model script in the scripts subdirectory in the cloned repository. The SAM weights will be wherever you saved them upon downloading them.\n\n\n\n\n\n\n\n\n\nAbsolute vs. relative file paths\n\n\n\nAbsolute file paths are generally recommended to avoid unintended behavior but will vary from computer to computer.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#dependencies",
    "href": "content/docs/index.html#dependencies",
    "title": "Menhaden Ageing Model",
    "section": "Dependencies",
    "text": "Dependencies\n\nAnaconda (for virtual environment implementation)\nGit CLI (for repository cloning)\nPython &gt;= 3.9\ntorch ==1.12.1\ntorchvision == 0.13.1\ntorchaudio == 0.12.1\nopencv-python\npandas\ntqdm\n\n\nRecommendations\n\nmatplotlib (data plotting in Python)\njupyter (viewing Jupyter notebook demonstrations provided in the GitHub repo)\n\nSee Getting Started for installation and setup instructions.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/index.html#release-notes",
    "href": "content/docs/index.html#release-notes",
    "title": "Menhaden Ageing Model",
    "section": "Release Notes",
    "text": "Release Notes\n\n\n Version History \n\n\n\n2025.0.1 (BETA) (May 2025): Initial version for testing with the following functionality:\n\n\nRun via command line with image, output, and model directories passed as required arguments\n\n\n&lt;/li&gt;\n&lt;li&gt; &lt;b&gt;2025.0.2 (BETA)&lt;/b&gt; (July 2025):\n  &lt;ul&gt;\n    &lt;li&gt; Adds optional histogram normalization (simple histogram equalization and Contrast Limited Adaptive Histogram Equalization) &lt;/li&gt;\n    &lt;li&gt; Separates image pre-processing and scale ageing into two separate scripts &lt;/li&gt;\n    &lt;li&gt; Model settings and hyperparameters, including input and output file paths, are now listed in a configuration YAML file passed as a single command line argument to both Python scripts &lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/li&gt;\n\n\n\n\n\n\n\n\n\nLicense\n\n\n\nSoftware code created by U.S. Government employees is not subject to copyright in the United States (17 U.S.C. §105). The United States/Department of Commerce reserve all rights to seek and obtain copyright protection in countries other than the United States for Software authored in its entirety by the Department of Commerce. To this end, the Department of Commerce hereby grants to Recipient a royalty-free, nonexclusive license to use, copy, and create derivative works of the Software outside of the United States.\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nThis software is a scientific product and is not official communication of the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All NOAA software and project code is provided on an “as is” basis and the user assumes responsibility for its use. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this software will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model"
    ]
  },
  {
    "objectID": "content/docs/suggestions.html",
    "href": "content/docs/suggestions.html",
    "title": "Next Steps",
    "section": "",
    "text": "Naming conventions\nThe example execution commands illustrate the importance of giving careful consideration to file and folder names. For example, the script name Scale_Ageing_Inference_Script_Image_Only.py is difficult and time-consuming to type, both due its length and its use of capital letters and underscores, both of which require using two keys. While using the shift key may seem trivial, it can quickly become an annoyance when repeatedly tying commands.\nIt is therefore recommended that the name of the script be shortened. An example might be inference-images.py.\nSimilarly, long directory names are equally burdensome to type. Even more annoying is having to handle special characters (including spaces), since different operating systems handle them differently. Special characters are even trickier because they would need to be handled properly in the code. It is best to avoid them altogether.\nIn general, best practice is to avoid spaces and special characters altogether in both file and directory names. Consider using camelCase instead, where the first letter of each word (except the first word) is capitalized. Underscores and dashes are also commonly used in directory names. These are less frowned-upon than spaces because they do not need to be escaped.\n\n\nAdditional scripts\nAdditional scripts, such as training scripts, should be added to the repo and this documentation once available.\n\n\nDemonstration notebooks\nOnce complete, Jupyter notebooks demonstrating the model may be added to this documentation.",
    "crumbs": [
      "Read the Docs",
      "Menhaden Ageing Model",
      "Next Steps"
    ]
  }
]